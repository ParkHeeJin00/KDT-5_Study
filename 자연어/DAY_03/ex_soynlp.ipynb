{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Soynlp] 학습 기반 토크나이저 <hr>\n",
    "- 품사 태깅, 단어 토큰화 등을 지원하는 단어 토크나이저\n",
    "- 비지도 학습으로 단어 토큰화 ==> 데이터에 자주 등장하는 단어들을 단어로 분석\n",
    "- 내부적으로 단어 점수 표로 동작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install soynlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [기존 형태소 분석기] : 신조어나 형태소 분석기에 등록되지 않은 단어 제대로 구분하지 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['에이', '비식스', '이대', '휘', '1월', '최애', '돌', '기부', '요정', '입니다', '.']\n",
      "['에이', '비식스', '이대', '휘', '1월', '최애', '돌', '기부', '요정', '이다', '.']\n",
      "['에이', '비식스', '이대', '휘', '1월', '최애', '돌', '기부', '요정', '입니다', '.']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "tokenizer = Okt()\n",
    "\n",
    "print(tokenizer.morphs('에이비식스 이대휘 1월 최애돌 기부 요정 입니다.'))\n",
    "\n",
    "# 형태소 분석시 매개변수 stem = True 설정 : 어간추출 -> 원형을 찾아줌\n",
    "print(tokenizer.morphs('에이비식스 이대휘 1월 최애돌 기부 요정 입니다.',stem = True)) # stem = True -> 어간추출 : 원형을 찾아줌\n",
    "# 형태소 분석시 매개변수 norm = True 설정 : 정규화 유무\n",
    "print(tokenizer.morphs('에이비식스 이대휘 1월 최애돌 기부 요정 입니다.', norm = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Sonlpy] 사용 => 말뭉치 데이터셋으로 학습 후 형태소 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "filename = '../data/text_data.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# => 학습 데이터 처리\n",
    "from soynlp import DoublespaceLineCorpus # 한개로 통합된 문서 데이터 분리\n",
    "from soynlp.word import WordExtractor # 단어 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 훈련 데이터 문서 : 30091\n"
     ]
    }
   ],
   "source": [
    "# => 훈련 데이터 문서 분리\n",
    "corpus = DoublespaceLineCorpus(filename)\n",
    "print(f' 훈련 데이터 문서 : {len(corpus)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.023 Gb\n"
     ]
    }
   ],
   "source": [
    "# => Sonlpy 학습 진행\n",
    "word_extractor = WordExtractor()\n",
    "# 학습 진행하며 단어별 점수\n",
    "word_extractor.train(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all cohesion probabilities was computed. # words = 223348\n",
      "all branching entropies was computed # words = 361598\n",
      "all accessor variety was computed # words = 361598\n"
     ]
    }
   ],
   "source": [
    "# 단어별 점수표 추출\n",
    "word_score_table = word_extractor.extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] - 샐\n",
      "[1] - 딥\n",
      "[2] - 논\n",
      "[3] - 쇠\n",
      "[4] - 멜\n",
      "[5] - 믿\n",
      "[6] - 섣\n",
      "[7] - 용\n",
      "[8] - 랄\n",
      "[9] - 염\n",
      "[10] - 잤\n",
      "[11] - 웹\n",
      "[12] - 룹\n",
      "[13] - 높\n",
      "[14] - 옵\n",
      "[15] - 벨\n",
      "[16] - 껏\n",
      "[17] - 슘\n",
      "[18] - 찾\n",
      "[19] - 줬\n",
      "[20] - 구\n",
      "[21] - 팸\n",
      "[22] - 러\n",
      "[23] - 렛\n",
      "[24] - 트\n",
      "[25] - 죽\n",
      "[26] - 낌\n",
      "[27] - 율\n",
      "[28] - 띕\n",
      "[29] - 캘\n",
      "[30] - 봅\n"
     ]
    }
   ],
   "source": [
    "# 단어 별 점수표 확인\n",
    "for idx, key in enumerate(word_score_table.keys()):\n",
    "    print(f'[{idx}] - {key}')\n",
    "    if idx ==30:break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 응집확률 (cohesion probability): 내부 문자열 (substring)이 얼마나 응집하여 자주 등장하는지를 판단하는 척도\n",
    "# 원리 : 문자열을 문자 단위로 분리, 왼쪽부터 순서대로 문자를 추가\n",
    "# 각 문자열이 주어졌을 때 그 다음 문자가 나올 확률을 계사 / 누적곱 한 값\n",
    "# 값이 높을수록 : 전체 코퍼스(내가 학습시킨 데이터)에서 이 문자열 시퀀스는 하나의 단어로 등장할 가능성이 높음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06393648140409527"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 확률\n",
    "word_score_table['바다'].cohesion_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11518621707955429"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_score_table['바다에'].cohesion_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# soynlpd의 L tokenizer\n",
    "# 띄어쓰기 단위로 나눈 어절 토큰 : L 토큰 + R 토큰\n",
    "# 예 ) 공원에 -> 공원 + 에 / 공부하는 -> 공부 + 하는\n",
    "# 분리 기준 : 점수가 가장 높은 L 토큰을 찾아내는 원리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('국제사회', '와'), ('우리', '의'), ('노력', '들로'), ('범죄', '를'), ('척결', '하자')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from soynlp.tokenizer import LTokenizer\n",
    "\n",
    "# 토큰으로 쪼개기 위한 L토큰 기준값\n",
    "# 단어를 키로 빼고, 응집력 점수만 밸류 값으로 설정\n",
    "scores = {word:score.cohesion_forward for word, score in word_score_table.items()}\n",
    "\n",
    "l_tokenizer = LTokenizer(scores= scores)\n",
    "l_tokenizer.tokenize('국제사회와 우리의 노력들로 범죄를 척결하자', flatten= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['국제사회', '와', '우리', '의', '노력', '들로', '범죄', '를', '척결', '하자']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 최대 점수 토크나이저\n",
    "# 띄어쓰기가 되지 않는 문장에서 점수가 높은 글자 시퀀스를 순차적으로 찾아내는 토크나이저\n",
    "# 띄어쓰기가 되어 있지 않은 문장을 넣어서 점수를 통해 토큰화 된 결과\n",
    "from soynlp.tokenizer import MaxScoreTokenizer\n",
    "\n",
    "maxscore_tokenizer = MaxScoreTokenizer(scores= scores)\n",
    "# 공백도 없고, 구두점도 없으면 토큰으로 쪼개지 못함.\n",
    "# 한 글자씩 쪼개서 확률값을 보고 단어를 예상 -> 반환해줌\n",
    "# 통계를 기반으로 단어를 추출\n",
    "# ex ) n 다음에 올 글자는 많고 불확실성이 높다. 그렇지만 net 다음에 올 글자는 제한되고 앞보다 불확실성이 낮다.\n",
    "# forword - 뒤에 붙을 글자 추측 / backwort - 앞에 붙을 글자 추측\n",
    "# 명사에다 조사를 추측할땐, 조사는 종류가 많으니까 불확실성이 올라가지만, 조사에다 명사를 추측할 때는 거곳보다 불확실성이 낮아짐.\n",
    "maxscore_tokenizer.tokenize('국제사회와우리의노력들로범죄를척결하자')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " 0.07856876882976202,\n",
       " 0.09217735975351507,\n",
       " 0.20075093164820865,\n",
       " 0.17387399904982392)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 가장 높은 확률을 가지는 합친 단어 선택\n",
    "word_score_table['국'].cohesion_forward, word_score_table['국제'].cohesion_forward, word_score_table['국제사'].cohesion_forward, word_score_table['국제사회'].cohesion_forward, word_score_table['국제사회와'].cohesion_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# soynlp를 이용한 반복되는 문자 정제\n",
    "# ㅋㅋ, ㅎㅎ 등의 이모티콘의 경우 불필요하게 연속되는 경우 많음\n",
    "# ㅋㅋ, ㅋㅋㅋ, ㅋㅋㅋㅋ와 같은 경우를 모두 서로 다른 단어로 처리하는 것은 불필요\n",
    "# 반복되는 것은 하나로 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "앜^^^^^^^^^\n",
      "아ㅋ영화존잼쓰ㅠ\n",
      "아ㅋ영화존잼쓰ㅠ\n",
      "아ㅋㅋ영화존잼쓰ㅠㅠ\n",
      "아ㅋㅋ영화존잼쓰ㅠㅠ\n"
     ]
    }
   ],
   "source": [
    "from soynlp.normalizer import *\n",
    "import string\n",
    "\n",
    "# 구두점 미리 제거\n",
    "print(string.punctuation)\n",
    "\n",
    "print(emoticon_normalize('앜^^^^^^^^^',num_repeats=1)) # 반복되는 ㅋ, ㅠ 하나로 처리\n",
    "print(emoticon_normalize('앜ㅋㅋㅋㅋㅋㅋ이영화존잼쓰ㅠㅠㅠㅠㅠ',num_repeats=1)) # 반복되는 ㅋ, ㅠ 하나로 처리\n",
    "print(emoticon_normalize('앜ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ이영화존잼쓰ㅠㅠㅠㅠㅠ',num_repeats=1)) # 반복되는 ㅋ, ㅠ 두개로 처리\n",
    "print(emoticon_normalize('앜ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ이영화존잼쓰ㅠㅠㅠㅠㅠ',num_repeats=2))\n",
    "print(emoticon_normalize('앜ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ이영화존잼쓰ㅠㅠㅠㅠㅠㅠ',num_repeats=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting customized_konlpy\n",
      "  Downloading customized_konlpy-0.0.64-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: Jpype1>=0.6.1 in c:\\programdata\\anaconda3\\envs\\torch_nlp38\\lib\\site-packages (from customized_konlpy) (1.4.1)\n",
      "Requirement already satisfied: konlpy>=0.4.4 in c:\\programdata\\anaconda3\\envs\\torch_nlp38\\lib\\site-packages (from customized_konlpy) (0.6.0)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\envs\\torch_nlp38\\lib\\site-packages (from Jpype1>=0.6.1->customized_konlpy) (23.2)\n",
      "Requirement already satisfied: lxml>=4.1.0 in c:\\programdata\\anaconda3\\envs\\torch_nlp38\\lib\\site-packages (from konlpy>=0.4.4->customized_konlpy) (5.1.0)\n",
      "Requirement already satisfied: numpy>=1.6 in c:\\programdata\\anaconda3\\envs\\torch_nlp38\\lib\\site-packages (from konlpy>=0.4.4->customized_konlpy) (1.24.3)\n",
      "Downloading customized_konlpy-0.0.64-py3-none-any.whl (881 kB)\n",
      "   ---------------------------------------- 0.0/881.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 10.2/881.5 kB ? eta -:--:--\n",
      "   - ------------------------------------- 41.0/881.5 kB 393.8 kB/s eta 0:00:03\n",
      "   ---- --------------------------------- 112.6/881.5 kB 930.9 kB/s eta 0:00:01\n",
      "   --------- ------------------------------ 204.8/881.5 kB 1.1 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 368.6/881.5 kB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 624.6/881.5 kB 2.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 881.5/881.5 kB 2.8 MB/s eta 0:00:00\n",
      "Installing collected packages: customized_konlpy\n",
      "Successfully installed customized_konlpy-0.0.64\n"
     ]
    }
   ],
   "source": [
    "# !pip install customized_konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\TORCH_NLP38\\lib\\site-packages\\konlpy\\tag\\_okt.py:17: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['은', '경이', '는', '사무실', '로', '갔습니다', '.']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ckonlpy.tag import Twitter\n",
    "\n",
    "# 말뭉치를 가지고 존재하는 단어를 가지고 형태소 분석해줌 -> 신조어나 이름, 조사는 분리 제대로 못함\n",
    "twitter = Twitter()\n",
    "\n",
    "twitter.morphs('은경이는 사무실로 갔습니다.')  # 은경이는 이름인데 하나씩 다 분리됨 -> 사전에 단어 추가 -> 해결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형태소 분석기에 사전 추가\n",
    "twitter.add_dictionary('은경이',tag = 'Noun')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['은경이', '는', '사무실', '로', '갔습니다', '.']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter.morphs('은경이는 사무실로 갔습니다.') # 은경이가 더이상 나누어지지 않음!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TORCH_NLP38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
