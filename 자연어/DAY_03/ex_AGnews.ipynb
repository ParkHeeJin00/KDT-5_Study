{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torchtext 라이브러리로 텍스트 분류 <hr>\n",
    "- 1단계 - 데이터 전처리 : 숫자형식으로 변환하는 것 까지\n",
    "- 2단계 - 모델 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1-1] 데이터 준비 => 내장 데이터셋 활용\n",
    "- AG_NEWS 데이터셋 반복자 : 라벨(label) + 문장의 튜플(tuple) 형태 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.datasets import AG_NEWS\n",
    "\n",
    "# DataPipe 타입 >>> iterator 타입 형변환\n",
    "train_iter = iter(AG_NEWS(split='train'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,\n",
       " \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 확인 => (label, text), label 1 ~ 4\n",
    "next(train_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[2] 데이터 처리 파이프라인 준비\n",
    "- 어휘집합(Vocab) 생성, 단어벡터(word vector), 토크나이저(tokenizer)\n",
    "- 가공되지 않은 텍스트 문자열에 대한 데이터 처리 빌딩 블록\n",
    "- 일반적인 NLP 데이터 처리\n",
    "    * 첫번째 단계 : 가공되지 않은 학습 데이터셋으로 어휘집 생성\n",
    "        - 토큰 목록 또는 반복자 받는 내장 핵토리 함수(factory function) : ``build_vocab_from_iterator``\n",
    "        - 사용자는 어휘집에 추가할 특수 기호(special symbol) 전달 가능\n",
    "    * 두번째 단계 : 어휘집 사용하여 문장 토큰화\n",
    "    * 세번째 단계 : 문장 토큰화 결과를 문자열 형태로 변환\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AG_NEWS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m get_tokenizer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbasic_english\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# 뉴스 학습 데이터 추출\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m train_iter \u001b[38;5;241m=\u001b[39m \u001b[43mAG_NEWS\u001b[49m(split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'AG_NEWS' is not defined"
     ]
    }
   ],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "# 토커나이즈 생성\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "# 뉴스 학습 데이터 추출\n",
    "train_iter = AG_NEWS(split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## => 토큰 생성 제너레이터 함수 : 데이터 추출하여 토큰화\n",
    "def yield_tokens(data_iter):\n",
    "    # 라벨, 텍스트 -> 텍스트 토큰화( 문장마다 들어가는 단어를 쪼갠 것 )\n",
    "    for _, txt in data_iter:\n",
    "        yield tokenizer(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'build_vocab_from_iterator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m## 단어사전 생성\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m vocab \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_vocab_from_iterator\u001b[49m(yield_tokens(train_iter), specials\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<UNK>\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;66;03m# min_freq = 기본값 : 1\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m## <UNK> 인덱스 0으로 설정\u001b[39;00m\n\u001b[0;32m      5\u001b[0m vocab\u001b[38;5;241m.\u001b[39mset_default_index(vocab[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<UNK>\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'build_vocab_from_iterator' is not defined"
     ]
    }
   ],
   "source": [
    "## 단어사전 생성\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<UNK>\"]) # min_freq = 기본값 : 1\n",
    "\n",
    "## <UNK> 인덱스 0으로 설정\n",
    "vocab.set_default_index(vocab[\"<UNK>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 475, 21, 0, 5297]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab(['<UNK>','here','is',' an','example'])\n",
    "# 단어사전에다가 단어를 넣었을때 인덱스 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 >> 정수 인코딩\n",
    "text_pipeline = lambda x:vocab(tokenizer(x))\n",
    "\n",
    "# 레이블 >> 정수 인코딩( 0 ~ 3 )\n",
    "label_pipeline = lambda x: int(x) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 배치 크기만큼 데이터셋 반환 함수\n",
    "def collate_batch(batch):\n",
    "    '''\n",
    "    batch : (label, text)\n",
    "    '''\n",
    "    # 배치 크기만큼의 라벨 텍스트 오프셋 값 저장 변수\n",
    "    label_list, text_list, offsets = [], [], [0] # offset : 첫번째 데이터의 갯수\n",
    "    \n",
    "    # 뉴스기사 1개씩 라벨 추출해서 저장\n",
    "    for (_label, _text) in batch:\n",
    "        \n",
    "        # 라벨 인코딩 후 저장\n",
    "        label_list.append(label_pipeline(_label))\n",
    "\n",
    "        # 텍스트 인코딩 후 저장\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "\n",
    "        # 텍스트 offset 즉, 텍스트 크기/길이 저장\n",
    "        offsets.append(processed_text.size(0))\n",
    " \n",
    "    # 텐서화 진행\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0) # 누적값 -> 전체 갯수\n",
    "    text_list = torch.cat(text_list)\n",
    "    \n",
    "    return label_list.to(device), text_list.to(device), offsets.to(device)\n",
    "\n",
    "\n",
    "# 이렇게 할땐 패딩 => embeddingbag 사용 - 길이 다 다른 문장 알아서 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = AG_NEWS(split = 'train')\n",
    "dataloader = DataLoader(train_iter,\n",
    "                        batch_size = 8, # 기사를 8개 뽑아온다는 뜻 -> 각 기사마다 길이가 모두 다름\n",
    "                        shuffle = False,\n",
    "                        collate_fn = collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_class : 4   vocab_size : 95811\n"
     ]
    }
   ],
   "source": [
    "# 분류 클래스 수와 단어사전 개수\n",
    "num_class = len(set([label for (label, text) in train_iter]))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(f'num_class : {num_class}   vocab_size : {vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TORCH_NLP38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
