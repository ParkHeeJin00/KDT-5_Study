{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 자연어 데이터셋 전처리<hr>\n",
    "- Korpora 패키지 활용\n",
    "- 데이터셋 : 한국어 혐오 데이터셋 korean_hate_speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-29T00:56:09.517997300Z",
     "start_time": "2024-03-29T00:56:09.459487800Z"
    }
   },
   "outputs": [],
   "source": [
    "### 패키지 설치\n",
    "# !pip install Korpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-29T00:56:09.892815800Z",
     "start_time": "2024-03-29T00:56:09.488880500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.2.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 패키지 설치 확인\n",
    "import Korpora\n",
    "\n",
    "Korpora.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] 데이터 준비<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-29T00:56:14.706511Z",
     "start_time": "2024-03-29T00:56:09.890821800Z"
    }
   },
   "outputs": [],
   "source": [
    "### 데이터 다운로드\n",
    "from Korpora import Korpora\n",
    "\n",
    "# 데이터셋 파일명\n",
    "data_file = 'korean_hate_speech'\n",
    "\n",
    "# 다운로드 fetch\n",
    "# corpus = Korpora.fetch(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-29T00:56:26.327551200Z",
     "start_time": "2024-03-29T00:56:14.702526200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
      "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
      "\n",
      "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
      "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
      "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
      "\n",
      "    # Description\n",
      "    Authors :\n",
      "        - Jihyung Moon* (inmoonlight@github)\n",
      "        - Won Ik Cho* (warnikchow@github)\n",
      "        - Junbum Lee (beomi@github)\n",
      "        * equal contribution\n",
      "    Repository : https://github.com/kocohub/korean-hate-speech\n",
      "    References :\n",
      "        - Moon, J., Cho, W. I., & Lee, J. (2020). BEEP! Korean Corpus of Online News\n",
      "          Comments for Toxic Speech Detection. arXiv preprint arXiv:2005.12503.\n",
      "\n",
      "    We provide the first human-annotated Korean corpus for toxic speech detection and the large unlabeled corpus.\n",
      "    The data is comments from the Korean entertainment news aggregation platform.\n",
      "\n",
      "    # License\n",
      "    Creative Commons Attribution-ShareAlike 4.0 International License.\n",
      "    Visit here for detail : https://creativecommons.org/licenses/by-sa/4.0/\n",
      "\n",
      "[Korpora] Corpus `korean hate speech` is already installed at C:\\Users\\kdp\\Korpora\\korean_hate_speech\\unlabeled\\unlabeled_comments_1.txt\n",
      "[Korpora] Corpus `korean hate speech` is already installed at C:\\Users\\kdp\\Korpora\\korean_hate_speech\\unlabeled\\unlabeled_comments_2.txt\n",
      "[Korpora] Corpus `korean hate speech` is already installed at C:\\Users\\kdp\\Korpora\\korean_hate_speech\\unlabeled\\unlabeled_comments_3.txt\n",
      "[Korpora] Corpus `korean hate speech` is already installed at C:\\Users\\kdp\\Korpora\\korean_hate_speech\\unlabeled\\unlabeled_comments_4.txt\n",
      "[Korpora] Corpus `korean hate speech` is already installed at C:\\Users\\kdp\\Korpora\\korean_hate_speech\\unlabeled\\unlabeled_comments_5.txt\n",
      "[Korpora] Corpus `korean hate speech` is already installed at C:\\Users\\kdp\\Korpora\\korean_hate_speech\\news_title\\unlabeled_comments.news_title_1.txt\n",
      "[Korpora] Corpus `korean hate speech` is already installed at C:\\Users\\kdp\\Korpora\\korean_hate_speech\\news_title\\unlabeled_comments.news_title_2.txt\n",
      "[Korpora] Corpus `korean hate speech` is already installed at C:\\Users\\kdp\\Korpora\\korean_hate_speech\\news_title\\unlabeled_comments.news_title_3.txt\n",
      "[Korpora] Corpus `korean hate speech` is already installed at C:\\Users\\kdp\\Korpora\\korean_hate_speech\\news_title\\unlabeled_comments.news_title_4.txt\n",
      "[Korpora] Corpus `korean hate speech` is already installed at C:\\Users\\kdp\\Korpora\\korean_hate_speech\\news_title\\unlabeled_comments.news_title_5.txt\n",
      "[Korpora] Corpus `korean hate speech` is already installed at C:\\Users\\kdp\\Korpora\\korean_hate_speech\\news_title\\dev.news_title.txt\n",
      "[Korpora] Corpus `korean hate speech` is already installed at C:\\Users\\kdp\\Korpora\\korean_hate_speech\\news_title\\test.news_title.txt\n",
      "[Korpora] Corpus `korean hate speech` is already installed at C:\\Users\\kdp\\Korpora\\korean_hate_speech\\news_title\\train.news_title.txt\n",
      "[Korpora] Corpus `korean hate speech` is already installed at C:\\Users\\kdp\\Korpora\\korean_hate_speech\\labeled\\dev.tsv\n",
      "[Korpora] Corpus `korean hate speech` is already installed at C:\\Users\\kdp\\Korpora\\korean_hate_speech\\labeled\\train.tsv\n",
      "[Korpora] Corpus `korean hate speech` is already installed at C:\\Users\\kdp\\Korpora\\korean_hate_speech\\test.no_label.tsv\n"
     ]
    }
   ],
   "source": [
    "### 데이터 로딩\n",
    "corpus = Korpora.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-29T00:56:26.372400500Z",
     "start_time": "2024-03-29T00:56:26.327551200Z"
    }
   },
   "outputs": [],
   "source": [
    "### 데이터 확인\n",
    "hateDS = corpus.train # 학습용 데이터셋\n",
    "hate_valDS = corpus.dev # 검증용 데이터셋\n",
    "hate_testDS = corpus.test # 테스트용 데이터셋\n",
    "unlabelDS = corpus.unlabeled # 프리딕트용 데이터셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-29T00:56:26.406287Z",
     "start_time": "2024-03-29T00:56:26.353464700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KoreanHateSpeech.train: size=7896\n",
       "  - KoreanHateSpeech.train.texts : list[str]\n",
       "  - KoreanHateSpeech.train.titles : list[str]\n",
       "  - KoreanHateSpeech.train.gender_biases : list[str]\n",
       "  - KoreanHateSpeech.train.biases : list[str]\n",
       "  - KoreanHateSpeech.train.hates : list[str]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hateDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-29T00:56:26.490007200Z",
     "start_time": "2024-03-29T00:56:26.394327700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(현재 호텔주인 심정) 아18 난 마른하늘에 날벼락맞고 호텔망하게생겼는데 누군 계속 추모받네....\n",
      "\"밤새 조문 행렬…故 전미선, 동료들이 그리워하는 따뜻한 배우 [종합]\"\n",
      "False\n",
      "others\n",
      "hate\n"
     ]
    }
   ],
   "source": [
    "### 데이터 확인\n",
    "hateDS = corpus.train # 학습용 데이터셋\n",
    "hate_valDS = corpus.dev # 검증용 데이터셋\n",
    "hate_testDS = corpus.test # 테스트용 데이터셋\n",
    "unlabelDS = corpus.unlabeled # 프리딕트용 데이터셋"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[2] 데이터 전처리 <hr>\n",
    "- 주제에 따른 데이터 셋 선택 => 뉴스댓글 + 차별종류\n",
    "- 형태소 분석기 결정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-29T00:56:35.461921400Z",
     "start_time": "2024-03-29T00:56:26.414261Z"
    }
   },
   "outputs": [],
   "source": [
    "### 피쳐 데이터 추출\n",
    "textData = corpus.get_all_texts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### 데이터 파일 저장\n",
    "save_filename = '../data/hate_text.csv'\n",
    "with open(save_filename, mode = 'w', encoding='utf-8') as f:\n",
    "    for text in textData:\n",
    "        f.write(text + '\\n')        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp38-cp38-win_amd64.whl.metadata (8.3 kB)\n",
      "Downloading sentencepiece-0.2.0-cp38-cp38-win_amd64.whl (991 kB)\n",
      "   ---------------------------------------- 0.0/991.7 kB ? eta -:--:--\n",
      "   - -------------------------------------- 41.0/991.7 kB 1.9 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 450.6/991.7 kB 7.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  983.0/991.7 kB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 991.7/991.7 kB 7.0 MB/s eta 0:00:00\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[단일 문장 토큰화] : ['▁안녕', '하세요', '.', '▁저는', '▁자연', '어', '▁처리', '를', '▁학', '습', '하고', '▁있는', '▁학생', '입니다', '.']\n",
      "[여러 문장 토큰화] : [['▁안녕', '하세요', '.', '▁저는', '▁자연', '어', '▁처리', '를', '▁학', '습', '하고', '▁있는', '▁학생', '입니다', '.'], ['▁이렇게', '▁토크', '나이', '저', '가', '▁잘', '▁되는', '지', '▁확인', '해', '보', '겠습니다', '.']]\n",
      "[단일 문장 정수 인코딩 ] : [4598, 1073, 30679, 27291, 4090, 30737, 11333, 31124, 816, 31105, 329, 1537, 14796, 4935, 30679]\n",
      "[여러 문장 정수 인코딩 ] : [[4598, 1073, 30679, 27291, 4090, 30737, 11333, 31124, 816, 31105, 329, 1537, 14796, 4935, 30679], [1646, 3614, 2850, 30901, 30711, 290, 4116, 30690, 597, 30712, 30765, 3682, 30679]]\n",
      "[정수 인코딩에서 문장 변환] : ['안녕하세요. 저는 자연어 처리를 학습하고 있는 학생입니다.', '이렇게 토크나이저가 잘 되는지 확인해보겠습니다.']\n",
      "[하위 단어 토큰에서 문장 변환] 안녕하세요. 저는 자연어 처리를 학습하고 있는 학생입니다.\n"
     ]
    }
   ],
   "source": [
    "from sentencepiece import SentencePieceProcessor\n",
    "\n",
    "tokenizer = SentencePieceProcessor()\n",
    "\n",
    "tokenizer.load('korean_hate_speech.model')\n",
    "\n",
    "sentence = '안녕하세요. 저는 자연어 처리를 학습하고 있는 학생입니다.'\n",
    "sentences = ['안녕하세요. 저는 자연어 처리를 학습하고 있는 학생입니다.', '이렇게 토크나이저가 잘 되는지 확인해보겠습니다.']\n",
    "\n",
    "# 토크나이징\n",
    "tokenized = tokenizer.encode_as_pieces(sentence)\n",
    "tokenized_sentences = tokenizer.encode_as_pieces(sentences)\n",
    "print(f'[단일 문장 토큰화] : {tokenized}')\n",
    "print(f'[여러 문장 토큰화] : {tokenized_sentences}')\n",
    "\n",
    "# 인코딩\n",
    "encoded_sentence = tokenizer.encode_as_ids(sentence)\n",
    "encoded_sentences = tokenizer.encode_as_ids(sentences)\n",
    "print(f'[단일 문장 정수 인코딩 ] : {encoded_sentence}')\n",
    "print(f'[여러 문장 정수 인코딩 ] : {encoded_sentences}')\n",
    "\n",
    "# 문장 변환\n",
    "decode_ids = tokenizer.decode_ids(encoded_sentences)\n",
    "decode_pieces = tokenizer.decode_pieces(encoded_sentence)\n",
    "print(f'[정수 인코딩에서 문장 변환] : {decode_ids}')\n",
    "print(f'[하위 단어 토큰에서 문장 변환] {decode_pieces}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TORCH_NLP38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
