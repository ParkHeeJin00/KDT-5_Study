{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [순환신경망 RNN]<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 2  # cell 개수 ( = 커널/퍼셉트론 개수 )  \n",
    "# 각 cell마다 HS 존재 - 입력값의 가중치,절편 + HS의 가중치, 절편값이 존재\n",
    "# 첫번째 문장은 HS가 0 => 입력값의 가중치, 절편 => HS에 존재하는 값\n",
    "NUM_LAYERS = 1\n",
    "SEQ_LENGTH = 3\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "# 데이터 초기 Hidden state                                         \n",
    "input = torch.randn(BATCH_SIZE,SEQ_LENGTH,10) # 입력 데이터( 배치크기, 시퀀스길이, 피쳐길이(input size) )\n",
    "# 문장이 3개의 단어로 이루어져 있는 하나의 문장을 받음\n",
    "# 패딩할 때, 지정했던 길이 => 시퀀스 길이\n",
    "# 보통 원핫 인코딩 하고 임베딩한 피쳐의 갯수 => 피쳐 길이\n",
    "# 시퀀스 길이가 3이고 피쳐 길이가 10이면 입력 데이터는 3x10 행렬\n",
    "\n",
    "\n",
    "# 첫번째 Hidden state 초기값\n",
    "h0 = torch.randn(NUM_LAYERS,BATCH_SIZE,HIDDEN_SIZE)     # 히든 초기값( 양방향 * 층수, 배치 크기, 히든 개수)\n",
    "# 층마다 히든이 있으면 층마다 초기화 해줘야함\n",
    "\n",
    "# RNN 인스턴스 생성\n",
    "rnn = nn.RNN(input_size=10, hidden_size=HIDDEN_SIZE, num_layers=NUM_LAYERS, batch_first=True)\n",
    "# 기본으로 AF tanh 사용( 또는 relu )\n",
    "\n",
    "#RNN 출력 ( 결과값, 히든스테이트값 )\n",
    "output, hn = rnn(input, h0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RNN 출력 - input data] torch.Size([1, 3, 10])  3D\n",
      "tensor([[[-1.0932, -1.0382, -0.4071,  1.7937, -0.3055,  0.4012,  0.2573,\n",
      "          -0.3577, -1.4618,  0.5991],\n",
      "         [-1.8893, -0.7111, -1.7980, -0.4195, -0.3584,  1.1719, -0.9229,\n",
      "          -0.2522,  0.7272, -0.1526],\n",
      "         [-3.0477, -1.1421, -0.5891, -0.5611,  0.0883, -0.4310, -1.9254,\n",
      "           1.8629, -0.6252,  1.1652]]])\n"
     ]
    }
   ],
   "source": [
    "print(f'[RNN 출력 - input data] {input.shape}  {input.ndim}D')\n",
    "print(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RNN 출력 - output data] torch.Size([1, 3, 2])  3D\n",
      "tensor([[[-0.9938,  0.9863],\n",
      "         [-0.9599,  0.9731],\n",
      "         [-0.4552,  0.9932]]], grad_fn=<TransposeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "print(f'[RNN 출력 - output data] {output.shape}  {output.ndim}D')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RNN 출력 - hidden state] torch.Size([1, 1, 2])  3D\n",
      "tensor([[[-0.4552,  0.9932]]], grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f'[RNN 출력 - hidden state] {hn.shape}  {hn.ndim}D')\n",
    "print(hn) # output의 마지막 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RNN 파라미터]\n",
      "---------[weight_ih_l0] \n",
      "Parameter containing:\n",
      "tensor([[ 0.3750, -0.1422,  0.1108, -0.3732,  0.0034, -0.5063, -0.1483,  0.4626,\n",
      "          0.5617,  0.3370],\n",
      "        [-0.7035,  0.3641,  0.0038,  0.6470,  0.0638,  0.4906, -0.5043, -0.1188,\n",
      "         -0.1309,  0.3380]], requires_grad=True)]\n",
      "---------[weight_hh_l0] \n",
      "Parameter containing:\n",
      "tensor([[0.6482, 0.4896],\n",
      "        [0.2383, 0.4579]], requires_grad=True)]\n",
      "---------[bias_ih_l0] \n",
      "Parameter containing:\n",
      "tensor([-0.6712, -0.3142], requires_grad=True)]\n",
      "---------[bias_hh_l0] \n",
      "Parameter containing:\n",
      "tensor([-0.2452,  0.5541], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "print('[RNN 파라미터]')\n",
    "for name, param in rnn.named_parameters():\n",
    "    print(f'---------[{name}] \\n{param}]') \n",
    "# 같은 층에 있는 HS끼리 상호작용해서 절편 계산 ex) HIDDEN_SIZE = 3 : bias 수 = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[all_weights] : 1\n",
      "[[Parameter containing:\n",
      "tensor([[ 0.4814,  0.5137,  0.2559, -0.2912, -0.0312,  0.6467,  0.7966,  0.3984,\n",
      "          0.4362,  0.6023]], requires_grad=True), Parameter containing:\n",
      "tensor([[-0.6756]], requires_grad=True), Parameter containing:\n",
      "tensor([0.2076], requires_grad=True), Parameter containing:\n",
      "tensor([0.5893], requires_grad=True)]]\n"
     ]
    }
   ],
   "source": [
    "# rnn 모델의 속성 출력\n",
    "print(f'[all_weights] : {len(rnn.all_weights)}')\n",
    "print(rnn.all_weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TORCH_NLP38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
