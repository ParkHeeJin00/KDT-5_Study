{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torchtext 라이브러리로 텍스트 분류 <hr>\n",
    "- 1단계 - 데이터 전처리 : 숫자형식으로 변환하는 것 까지\n",
    "- 2단계 - 모델 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1-1] 데이터 준비 => 내장 데이터셋 활용\n",
    "- AG_NEWS 데이터셋 반복자 : 라벨(label) + 문장의 튜플(tuple) 형태 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.datasets import AG_NEWS\n",
    "\n",
    "# DataPipe 타입 >>> iterator 타입 형변환\n",
    "train_iter = iter(AG_NEWS(split='train'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torchdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,\n",
       " \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 확인 => (label, text), label 1 ~ 4\n",
    "next(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "# 토커나이즈 생성\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "# 뉴스 학습 데이터 추출\n",
    "train_iter = AG_NEWS(split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## => 토큰 생성 제너레이터 함수 : 데이터 추출하여 토큰화\n",
    "def yield_tokens(data_iter):\n",
    "    # 라벨, 텍스트 -> 텍스트 토큰화( 문장마다 들어가는 단어를 쪼갠 것 )\n",
    "    for _, txt in data_iter:\n",
    "        yield tokenizer(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 단어사전 생성\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<UNK>\"]) # min_freq = 기본값 : 1\n",
    "\n",
    "## <UNK> 인덱스 0으로 설정\n",
    "vocab.set_default_index(vocab[\"<UNK>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 475, 21, 0, 5297]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab(['<UNK>','here','is',' an','example'])\n",
    "# 단어사전에다가 단어를 넣었을때 인덱스 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 >> 정수 인코딩\n",
    "text_pipeline = lambda x:vocab(tokenizer(x))\n",
    "\n",
    "# 레이블 >> 정수 인코딩( 0 ~ 3 )\n",
    "label_pipeline = lambda x: int(x) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 배치 크기만큼 데이터셋 반환 함수\n",
    "def collate_batch(batch):\n",
    "    '''\n",
    "    batch : (label, text)\n",
    "    '''\n",
    "    # 배치 크기만큼의 라벨 텍스트 오프셋 값 저장 변수\n",
    "    label_list, text_list, offsets = [], [], [0] # offset : 첫번째 데이터의 갯수\n",
    "    \n",
    "    # 뉴스기사 1개씩 라벨 추출해서 저장\n",
    "    for (_label, _text) in batch:\n",
    "        \n",
    "        # 라벨 인코딩 후 저장\n",
    "        label_list.append(label_pipeline(_label))\n",
    "\n",
    "        # 텍스트 인코딩 후 저장\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "\n",
    "        # 텍스트 offset 즉, 텍스트 크기/길이 저장\n",
    "        offsets.append(processed_text.size(0))\n",
    "\n",
    "    # 텐서화 진행\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0) # 누적값 -> 전체 갯수\n",
    "    text_list = torch.cat(text_list)\n",
    "    \n",
    "    return label_list.to(device), text_list.to(device), offsets.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = AG_NEWS(split = 'train')\n",
    "dataloader = DataLoader(train_iter,\n",
    "                        batch_size = 8, # 기사를 8개 뽑아온다는 뜻 -> 각 기사마다 길이가 모두 다름\n",
    "                        shuffle = False,\n",
    "                        collate_fn = collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_class : 4   vocab_size : 95811\n"
     ]
    }
   ],
   "source": [
    "# 분류 클래스 수와 단어사전 개수\n",
    "num_class = len(set([label for (label, text) in train_iter]))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(f'num_class : {num_class}   vocab_size : {vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 설계\n",
    "# 입력층 : EmbeddingBag Layer - 레이어와 분류 목적을 위한 선형 레이어, 텍스트 길이는 오프셋\n",
    "# 은닉층 : Linear - 4개 클래스 분류\n",
    "class TextModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_class):\n",
    "        super(TextModel, self).__init__()\n",
    "        # 모델 구성 층 정의 - 패딩을 안했을때 EmbeddingBag 사용\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embedding_dim, sparse=False)\n",
    "        self.fc = nn.Linear(embedding_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    # 가중치 초기화\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        # uniform : 균등분포\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    # 순방향 학습 진행\n",
    "    def forward(self,text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 3\n",
    "EMBEDDING_DIM = 64\n",
    "VOCAB_SIZE = len(vocab)\n",
    "NUM_CLASS = len(set([label for (label, text) in train_iter]))\n",
    "EPOCHS = 10\n",
    "LR = 5\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 관련 인스턴스\n",
    "MODEL = TextModel(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_SIZE, NUM_CLASS).to(device)\n",
    "\n",
    "CRITERION = nn.CrossEntropyLoss()\n",
    "OPTIMIZER = torch.optim.SGD(MODEL.parameters(), lr=LR)\n",
    "SCHEDULER = torch.optim.lr_scheduler.StepLR(OPTIMIZER, 1.0, gamma=0.1) # gamma : step마다 lr을 줄이는 간격"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습관련 함수들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 관련 함수 정의 \n",
    "\n",
    "\n",
    "def train(model, dataloader, optimizer, criterion, epoch):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    # 학습 평가 관련 변수들\n",
    "    total_acc, total_count = 0,0\n",
    "    log_interval=300\n",
    "    \n",
    "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "        predicted_label = model(text, offsets)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(predicted_label, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 배치 학습 평가\n",
    "        total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        \n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            print(f\"epoch : {epoch} batch : {idx} loss : {loss.item()}\")\n",
    "            print(f\"Accuracy : {total_acc/total_count}\")\n",
    "            total_acc, total_count = 0,0\n",
    "            \n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    \n",
    "    total_acc, total_count = 0,0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "            predicted_label = model(text, offsets)\n",
    "            loss = criterion(predicted_label, label)\n",
    "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "            \n",
    "    return total_acc/total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, text, text_pipeline):\n",
    "    with torch.no_grad():\n",
    "        # 토큰화 > 정수 변환 > 텐서\n",
    "        text = torch.tensor(text_pipeline(text), dtype=torch.int64).to(device)\n",
    "        text = text.unsqueeze(0)\n",
    "        offsets = torch.tensor([0]).to(device)\n",
    "        predicted_label = model(text, offsets)\n",
    "        return predicted_label.argmax(1).item() + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 학습 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1 batch : 300 loss : 1.052221655845642\n",
      "Accuracy : 0.4676079734219269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mybae\\anaconda3\\envs\\TORCH17_NLP38\\lib\\site-packages\\torch\\utils\\data\\datapipes\\iter\\combining.py:337: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
      "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1 Accuracy : 0.429925\n",
      "epoch : 2 batch : 300 loss : 1.0132092237472534\n",
      "Accuracy : 0.6000830564784053\n",
      "epoch : 2 Accuracy : 0.5110666666666667\n",
      "epoch : 3 batch : 300 loss : 0.9934564828872681\n",
      "Accuracy : 0.6258305647840532\n",
      "epoch : 3 Accuracy : 0.521375\n",
      "epoch : 4 batch : 300 loss : 0.9926716685295105\n",
      "Accuracy : 0.651578073089701\n",
      "epoch : 4 Accuracy : 0.5203\n",
      "epoch : 5 batch : 300 loss : 0.9926028251647949\n",
      "Accuracy : 0.6503322259136213\n",
      "epoch : 5 Accuracy : 0.5202583333333334\n",
      "epoch : 6 batch : 300 loss : 0.9925960302352905\n",
      "Accuracy : 0.6503322259136213\n",
      "epoch : 6 Accuracy : 0.5202\n",
      "epoch : 7 batch : 300 loss : 0.9925957322120667\n",
      "Accuracy : 0.6503322259136213\n",
      "epoch : 7 Accuracy : 0.5201916666666667\n",
      "epoch : 8 batch : 300 loss : 0.9925956130027771\n",
      "Accuracy : 0.6503322259136213\n",
      "epoch : 8 Accuracy : 0.5201916666666667\n",
      "epoch : 9 batch : 300 loss : 0.9925955533981323\n",
      "Accuracy : 0.6503322259136213\n",
      "epoch : 9 Accuracy : 0.5201916666666667\n",
      "epoch : 10 batch : 300 loss : 0.9925955533981323\n",
      "Accuracy : 0.6503322259136213\n",
      "epoch : 10 Accuracy : 0.5201916666666667\n"
     ]
    }
   ],
   "source": [
    "# 학습 및 검증 진행\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    train(MODEL, dataloader, OPTIMIZER, CRITERION, epoch)\n",
    "    accu_val = evaluate(MODEL, dataloader, CRITERION)\n",
    "    print(f\"epoch : {epoch} Accuracy : {accu_val}\")\n",
    "    SCHEDULER.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TORCH_NLP38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
